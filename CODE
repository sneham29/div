import numpy as np
from Orange.data import Table, Domain, ContinuousVariable

def haar(x):
    r=[]
    for v in x:
        c=[]
        while len(v)>1:
            a,d=(v[::2]+v[1::2])/2,(v[::2]-v[1::2])/2
            c+=d.tolist();v=a
        r.append(c+v.tolist())
    return np.array(r)

out_data=Table(Domain([ContinuousVariable(f"H{i}") for i in range(len(in_data.domain))]), haar(in_data.X))


EXP 7:


EXP 8:
 
EXP 9:
CODE 1:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
X = np.array(in_data.X)
# Perform Agglomerative Clustering
model = AgglomerativeClustering(n_clusters=3)
labels = model.fit_predict(X)

# Scatter plot of clusters
plt.figure()
plt.scatter(X[:,0], X[:,1], c=labels, cmap='viridis', s=50)
plt.title("Agglomerative Clustering (Bottom-Up)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
CODE 2:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
X = np.array(in_data.X)
# Simulate Divisive Clustering (recursive top-down splitting)
labels = np.zeros(len(X))
k = 2
for i in range(k):
    km = KMeans(n_clusters=2, random_state=i).fit(X[labels==i])
    sub_labels = km.labels_ + 2*i
    labels[labels==i] = sub_labels

# Scatter plot
plt.figure()
plt.scatter(X[:,0], X[:,1], c=labels, cmap='plasma', s=50)
plt.title("Divisive Clustering (Top-Down)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()


EXP 1 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Convert Orange data to DataFrame (works for any dataset in Orange)
df = pd.DataFrame(in_data.X, columns=[a.name for a in in_data.domain.attributes])

# Add target column if available
if hasattr(in_data, 'Y') and len(in_data.Y) == len(df):
    df['target'] = [str(val) for val in in_data.Y]

# Automatically select first 3 numeric columns
numeric_cols = df.select_dtypes(include=[np.number]).columns[:3]

# Skip if fewer than 3 numeric columns
if len(numeric_cols) < 3:
    raise ValueError("Dataset must have at least 3 numeric columns for 3D visualization.")

# Fill missing values
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce').fillna(df.mean())

# --- 3D Bar Plot ---
fig = plt.figure(figsize=(8, 5))
ax = fig.add_subplot(111, projection='3d')

x = np.arange(len(df))
w = 0.2
colors = ['r', 'g', 'b']

for i, (col, color) in enumerate(zip(numeric_cols, colors)):
    ax.bar(x + i*w, df[col], zs=i, zdir='y', width=w, color=color, alpha=0.7, label=col)

ax.set_xlabel('Sample Index')
ax.set_ylabel('Feature')
ax.set_zlabel('Value')
ax.set_yticks(range(len(numeric_cols)))
ax.set_yticklabels(numeric_cols)
ax.set_title('3D Feature Visualization')

plt.legend()
plt.tight_layout()
plt.show()


EXP 10:
[9:24 am, 31/10/2025] VarshzzzzðŸ’™: import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans

# Convert Orange table to numpy
X = np.array(in_data.X)

# Perform MiniBatch K-Means
kmeans = MiniBatchKMeans(n_clusters=3, random_state=0, batch_size=10)
labels = kmeans.fit_predict(X)

# Visualize clusters
plt.scatter(X[:,0], X[:,1], c=labels, cmap='viridis', s=50)
plt.title("MiniBatch K-Means Clustering on Market Basket Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
[9:24 am, 31/10/2025] VarshzzzzðŸ’™: import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# Convert Orange table to pandas DataFrame
df = pd.DataFrame(in_data.X, columns=[v.name for v in in_data.domain.attributes])

# Convert to binary (if not already)
df = df.applymap(lambda x: 1 if x > 0 else 0)

# Run Apriori algorithm
frequent_items = apriori(df, min_support=0.1, use_colnames=True)
rules = association_rules(frequent_items, metric="confidence", min_threshold=0.7)

# Display selected columns
out_data = rules[["antecedents", "consequents", "support", "confidence", "lift"]]
